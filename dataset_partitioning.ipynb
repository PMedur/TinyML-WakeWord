{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c659e3a-411a-4c0b-b568-3aedc63b1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "    hash_name_hashed = hashlib.sha1(hash_name.encode('utf-8')).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                        (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                       (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result\n",
    "\n",
    "def partition_dataset(data_dir, validation_percentage, testing_percentage):\n",
    "    validation_list = []\n",
    "    testing_list = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                set_type = which_set(file_path, validation_percentage, testing_percentage)\n",
    "                if set_type == 'validation':\n",
    "                    validation_list.append(file_path)\n",
    "                elif set_type == 'testing':\n",
    "                    testing_list.append(file_path)\n",
    "    \n",
    "    return validation_list, testing_list\n",
    "\n",
    "def write_list_to_file(file_list, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in file_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Parameters\n",
    "data_dir = '/home/pmedur/strojnoUcenje/env/bin/TorchAudio/SpeechCommands/speech_commands_v0.02'  # Replace with the path to your wav files\n",
    "validation_percentage = 10.0\n",
    "testing_percentage = 10.0\n",
    "\n",
    "# Partition the dataset\n",
    "validation_list, testing_list = partition_dataset(data_dir, validation_percentage, testing_percentage)\n",
    "\n",
    "# Write the lists to files\n",
    "write_list_to_file(validation_list, 'validation_list.txt')\n",
    "write_list_to_file(testing_list, 'testing_list.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a19f65c-df16-49d6-b674-d5fc2444aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(hash_name, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\"\"\"\n",
    "    hash_name_hashed = hashlib.sha1(hash_name.encode('utf-8')).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                        (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                       (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        return 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        return 'testing'\n",
    "    else:\n",
    "        return 'training'\n",
    "\n",
    "def partition_dataset(data_dir, validation_percentage, testing_percentage):\n",
    "    label_speaker_files = {}\n",
    "\n",
    "    # Group files by label and speaker ID\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                base_name = os.path.basename(file_path)\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                speaker_id = re.sub(r'_nohash_.*$', '', base_name)\n",
    "\n",
    "                if label not in label_speaker_files:\n",
    "                    label_speaker_files[label] = {}\n",
    "                if speaker_id not in label_speaker_files[label]:\n",
    "                    label_speaker_files[label][speaker_id] = []\n",
    "                label_speaker_files[label][speaker_id].append(file_path)\n",
    "\n",
    "    validation_list = []\n",
    "    testing_list = []\n",
    "\n",
    "    # Randomly shuffle speaker IDs and assign to sets\n",
    "    for label, speaker_files in label_speaker_files.items():\n",
    "        speaker_ids = list(speaker_files.keys())\n",
    "        random.shuffle(speaker_ids)  # Randomize the order of speaker IDs\n",
    "\n",
    "        for speaker_id in speaker_ids:\n",
    "            files = speaker_files[speaker_id]\n",
    "            set_type = which_set(speaker_id, validation_percentage, testing_percentage)\n",
    "            if set_type == 'validation':\n",
    "                validation_list.extend(files)\n",
    "            elif set_type == 'testing':\n",
    "                testing_list.extend(files)\n",
    "\n",
    "    return validation_list, testing_list\n",
    "\n",
    "def write_list_to_file(file_list, file_path):\n",
    "    organized_entries = organize_file_entries(file_list)\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in organized_entries:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "def organize_file_entries(file_list):\n",
    "    organized_entries = []\n",
    "    for file_path in file_list:\n",
    "        base_name = os.path.basename(file_path)\n",
    "        label = os.path.basename(os.path.dirname(file_path))\n",
    "        speaker_id_match = re.search(r'([^_]*)_nohash_', base_name)\n",
    "        utterance_number_match = re.search(r'_nohash_([0-9]+)\\.wav$', base_name)\n",
    "        \n",
    "        if speaker_id_match and utterance_number_match:\n",
    "            speaker_id = speaker_id_match.group(1)\n",
    "            utterance_number = utterance_number_match.group(1)\n",
    "            organized_entries.append(f\"{label} {speaker_id} {utterance_number}\")\n",
    "        else:\n",
    "            print(f\"Filename format issue: {file_path}\")\n",
    "    return organized_entries\n",
    "\n",
    "# Parameters\n",
    "data_dir = '/home/pmedur/strojnoUcenje/env/bin/TorchAudio/SpeechCommands/speech_commands_v0.02'  # Replace with the path to your wav files\n",
    "validation_percentage = 10.0\n",
    "testing_percentage = 10.0\n",
    "\n",
    "# Seed the random number generator for reproducibility\n",
    "random.seed()\n",
    "\n",
    "# Partition the dataset\n",
    "validation_list, testing_list = partition_dataset(data_dir, validation_percentage, testing_percentage)\n",
    "\n",
    "# Write the lists to files\n",
    "write_list_to_file(validation_list, 'validation_list.txt')\n",
    "write_list_to_file(testing_list, 'testing_list.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ff85ba-97d9-443d-9889-233db2e02d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import python_speech_features\n",
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class SpeechDataLoader:\n",
    "    def __init__(self, dataset_path, feature_sets_file, sample_rate=8000, num_mfcc=16, len_mfcc=16, val_ratio=0.1, test_ratio=0.1, n_splits=2):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.feature_sets_file = feature_sets_file\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_mfcc = num_mfcc\n",
    "        self.len_mfcc = len_mfcc\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.n_splits = n_splits\n",
    "        self.target_list = None\n",
    "        self.data_by_id = None\n",
    "        self.filenames = None\n",
    "        self.y = None\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.x_val = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load target labels\n",
    "        self.target_list = [name for name in listdir(self.dataset_path) if isdir(join(self.dataset_path, name))]\n",
    "        self.target_list.remove('_background_noise_')\n",
    "        self.target_list.remove('.ipynb_checkpoints')\n",
    "\n",
    "        # Load filenames and labels, grouped by ID\n",
    "        self.data_by_id = defaultdict(list)\n",
    "        for index, target in enumerate(self.target_list):\n",
    "            files = listdir(join(self.dataset_path, target))\n",
    "            files = [f for f in files if f.endswith('.wav')]  # Filter out non-wav files\n",
    "            for file in files:\n",
    "                id_part = file.split('_')[0]\n",
    "                self.data_by_id[(target, id_part)].append((file, index))\n",
    "\n",
    "        # Convert data_by_id to lists\n",
    "        self.filenames = []\n",
    "        self.y = []\n",
    "        for (target, id_part), files in self.data_by_id.items():\n",
    "            for file, label in files:\n",
    "                self.filenames.append(join(target, file))\n",
    "                self.y.append(label)\n",
    "\n",
    "        # Shuffle data by ID\n",
    "        ids = list(self.data_by_id.keys())\n",
    "        random.shuffle(ids)\n",
    "        self.filenames = []\n",
    "        self.y = []\n",
    "        for (target, id_part) in ids:\n",
    "            for file, label in self.data_by_id[(target, id_part)]:\n",
    "                self.filenames.append(join(target, file))\n",
    "                self.y.append(label)\n",
    "\n",
    "    def extract_features(self, filenames, y_orig):\n",
    "        out_x = []\n",
    "        out_y = []\n",
    "\n",
    "        for index, filename in enumerate(filenames):\n",
    "            path = join(self.dataset_path, filename)\n",
    "            mfccs = self.calc_mfcc(path)\n",
    "\n",
    "            if mfccs.shape[1] == self.len_mfcc:\n",
    "                out_x.append(mfccs)\n",
    "                out_y.append(y_orig[index])\n",
    "\n",
    "        return out_x, out_y\n",
    "\n",
    "    def calc_mfcc(self, path):\n",
    "        signal, fs = librosa.load(path, sr=self.sample_rate)\n",
    "        mfccs = python_speech_features.base.mfcc(signal,\n",
    "                                                 samplerate=fs,\n",
    "                                                 winlen=0.256,\n",
    "                                                 winstep=0.050,\n",
    "                                                 numcep=self.num_mfcc,\n",
    "                                                 nfilt=26,\n",
    "                                                 nfft=2048,\n",
    "                                                 preemph=0.0,\n",
    "                                                 ceplifter=0,\n",
    "                                                 appendEnergy=False,\n",
    "                                                 winfunc=np.hanning)\n",
    "        return mfccs.transpose()\n",
    "\n",
    "    def k_fold_split(self):\n",
    "        \"\"\"\n",
    "        total_ids = len(self.data_by_id)\n",
    "        ids = list(self.data_by_id.keys())\n",
    "        random.shuffle(ids)\n",
    "        print(total_ids)\n",
    "        # Calculate the size of the data for further testing\n",
    "        further_test_size = int(total_ids * 0.05)\n",
    "        \n",
    "        # Take a portion of data for further testing\n",
    "        further_test_ids = ids[:further_test_size]\n",
    "        ids = ids[further_test_size:]\n",
    "        print(len(ids))\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=None)\n",
    "        folds = []\n",
    "        print(\"started folding\")\n",
    "        \n",
    "        for train_val_index, test_index in kf.split(ids):\n",
    "            train_val_ids = [ids[i] for i in train_val_index]\n",
    "            test_ids = [ids[i] for i in test_index]\n",
    "            print(len(train_val_index))\n",
    "            print(len(test_index))\n",
    "            # Determine the size of the validation and test sets based on the number of folds\n",
    "            val_size = int(len(train_val_ids) * self.val_ratio)\n",
    "            test_size = int(len(test_ids) * self.test_ratio)\n",
    "    \n",
    "            # Split the training and validation sets\n",
    "            val_ids = train_val_ids[:val_size]\n",
    "            train_ids = train_val_ids[val_size:]\n",
    "    \n",
    "            # Split the test set\n",
    "            test_ids = test_ids[:test_size]\n",
    "    \n",
    "            # Extract features for each partition\n",
    "            x_train_fold, y_train_fold = self.extract_partition(train_ids)\n",
    "            x_val_fold, y_val_fold = self.extract_partition(val_ids)\n",
    "            x_test_fold, y_test_fold = self.extract_partition(test_ids)\n",
    "    \n",
    "            # Append the fold to the list of folds\n",
    "            folds.append((x_train_fold, y_train_fold, x_val_fold, y_val_fold, x_test_fold, y_test_fold))\n",
    "            print(\"fold split made\")\n",
    "\n",
    "        # Save the further testing data\n",
    "        further_test_data = self.extract_partition(further_test_ids)\n",
    "        further_test_file = 'further_test_data.npz'\n",
    "        np.savez(further_test_file,\n",
    "                 x_test=further_test_data[0],\n",
    "                 y_test=further_test_data[1])\n",
    "        print(f\"Saved {further_test_file}\")\n",
    "        \n",
    "        return folds\n",
    "    \"\"\"\n",
    "        total_ids = len(self.data_by_id)\n",
    "        ids = list(self.data_by_id.keys())\n",
    "        random.shuffle(ids)\n",
    "    \n",
    "        # Calculate the size of the data for further testing\n",
    "        further_test_size = int(total_ids * 0.05)\n",
    "    \n",
    "        # Take a portion of data for further testing\n",
    "        further_test_ids = ids[:further_test_size]\n",
    "        remaining_ids = ids[further_test_size:]\n",
    "    \n",
    "        # KFold cross-validation\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "        folds = []\n",
    "        print(\"started folding\")\n",
    "        for train_val_index, test_index in kf.split(remaining_ids):\n",
    "            train_val_ids = [remaining_ids[i] for i in train_val_index]\n",
    "            test_ids = [remaining_ids[i] for i in test_index]\n",
    "    \n",
    "            # Split train_val_ids into training and validation sets\n",
    "            val_size = int(len(train_val_ids) * 0.1)\n",
    "            train_ids = train_val_ids[val_size:]\n",
    "            val_ids = train_val_ids[:val_size]\n",
    "    \n",
    "            # Extract features for each partition\n",
    "            x_train_fold, y_train_fold = self.extract_partition(train_ids)\n",
    "            x_val_fold, y_val_fold = self.extract_partition(val_ids)\n",
    "            x_test_fold, y_test_fold = self.extract_partition(test_ids)\n",
    "    \n",
    "            # Append the fold to the list of folds\n",
    "            folds.append((x_train_fold, y_train_fold, x_val_fold, y_val_fold, x_test_fold, y_test_fold))\n",
    "            print(\"fold split made\")\n",
    "    \n",
    "        # Save the further testing data\n",
    "        x_further_test, y_further_test = self.extract_partition(further_test_ids)\n",
    "        further_test_path = 'further_test_data.npz'\n",
    "        np.savez(further_test_path, x_test=x_further_test, y_test=y_further_test)\n",
    "\n",
    "        return folds\n",
    "\n",
    "    \"\"\"\n",
    "    def k_fold_split(self):\n",
    "        total_ids = len(self.data_by_id)\n",
    "        ids = list(self.data_by_id.keys())\n",
    "        random.shuffle(ids)\n",
    "\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=None)\n",
    "        folds = []\n",
    "        print(\"started folding\")\n",
    "        \n",
    "        for train_val_index, test_index in kf.split(ids):\n",
    "            train_val_ids = [ids[i] for i in train_val_index]\n",
    "            test_ids = [ids[i] for i in test_index]\n",
    "            \n",
    "            val_set_size = int(len(train_val_ids) * self.val_ratio / (1 - self.test_ratio))\n",
    "            random.shuffle(train_val_ids)\n",
    "            val_ids = train_val_ids[:val_set_size]\n",
    "            train_ids = train_val_ids[val_set_size:]\n",
    "            \n",
    "            x_train_fold, y_train_fold = self.extract_partition(train_ids)\n",
    "            x_val_fold, y_val_fold = self.extract_partition(val_ids)\n",
    "            x_test_fold, y_test_fold = self.extract_partition(test_ids)\n",
    "\n",
    "            folds.append((x_train_fold, y_train_fold, x_val_fold, y_val_fold, x_test_fold, y_test_fold))\n",
    "            print(\"Fold split made\")\n",
    "        return folds\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_partition(self, ids):\n",
    "        filenames = []\n",
    "        labels = []\n",
    "        for (target, id_part) in ids:\n",
    "            for file, label in self.data_by_id[(target, id_part)]:\n",
    "                filenames.append(join(target, file))\n",
    "                labels.append(label)\n",
    "        return self.extract_features(filenames, labels)\n",
    "\n",
    "    def save_folds(self, folds):\n",
    "        for i, (x_train_fold, y_train_fold, x_val_fold, y_val_fold, x_test_fold, y_test_fold) in enumerate(folds):\n",
    "            fold_file = f'fold_{i + 1}.npz'\n",
    "            np.savez(fold_file,\n",
    "                     x_train_fold=x_train_fold,\n",
    "                     y_train_fold=y_train_fold,\n",
    "                     x_val_fold=x_val_fold,\n",
    "                     y_val_fold=y_val_fold,\n",
    "                     x_test_fold=x_test_fold,\n",
    "                     y_test_fold=y_test_fold)\n",
    "            print(f\"Saved {fold_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a2270b-4bd3-4e66-958f-8dfb44d51447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started folding\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "fold split made\n",
      "Saved fold_1.npz\n",
      "Saved fold_2.npz\n",
      "Saved fold_3.npz\n",
      "Saved fold_4.npz\n",
      "Saved fold_5.npz\n",
      "Saved fold_6.npz\n",
      "Saved fold_7.npz\n",
      "Saved fold_8.npz\n",
      "Saved fold_9.npz\n",
      "Saved fold_10.npz\n"
     ]
    }
   ],
   "source": [
    "loader = SpeechDataLoader(dataset_path='/home/pmedur/strojnoUcenje/env/bin/TorchAudio/SpeechCommands/speech_commands_v0.02 (copy)',\n",
    "                          feature_sets_file='all_targets_mfcc_sets.npz')\n",
    "loader.load_data()\n",
    "folds = loader.k_fold_split()\n",
    "loader.save_folds(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2821afe9-46bc-48d9-84ae-24988ac1225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test']\n",
      "(77441, 16, 16)\n",
      "(9689, 16, 16)\n",
      "(9726, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "feature_sets_path = '/home/pmedur/strojnoUcenje/env/bin/Tensorflow_speech_recognition/tflite-speech-recognition-master'\n",
    "feature_sets_filename = 'all_targets_mfcc_sets.npz'\n",
    "feature_sets = np.load(join(feature_sets_path, feature_sets_filename))\n",
    "print(feature_sets.files)\n",
    "# Assigning feature sets\n",
    "x_train = feature_sets['x_train']\n",
    "y_train = feature_sets['y_train']\n",
    "x_val = feature_sets['x_val']\n",
    "y_val = feature_sets['y_val']\n",
    "x_test = feature_sets['x_test']\n",
    "y_test = feature_sets['y_test']\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7a5e86-c63e-4fcc-b8fd-bc513fafea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Training samples: 74661\n",
      "Training targets: 74661\n",
      "Validation samples: 8373\n",
      "Validation targets: 8373\n",
      "Testing samples: 9086\n",
      "Testing targets: 9086\n",
      "Fold 2\n",
      "Training samples: 74446\n",
      "Training targets: 74446\n",
      "Validation samples: 8351\n",
      "Validation targets: 8351\n",
      "Testing samples: 9323\n",
      "Testing targets: 9323\n",
      "Fold 3\n",
      "Training samples: 74395\n",
      "Training targets: 74395\n",
      "Validation samples: 8348\n",
      "Validation targets: 8348\n",
      "Testing samples: 9377\n",
      "Testing targets: 9377\n",
      "Fold 4\n",
      "Training samples: 74502\n",
      "Training targets: 74502\n",
      "Validation samples: 8375\n",
      "Validation targets: 8375\n",
      "Testing samples: 9243\n",
      "Testing targets: 9243\n",
      "Fold 5\n",
      "Training samples: 74387\n",
      "Training targets: 74387\n",
      "Validation samples: 8367\n",
      "Validation targets: 8367\n",
      "Testing samples: 9366\n",
      "Testing targets: 9366\n",
      "Fold 6\n",
      "Training samples: 74634\n",
      "Training targets: 74634\n",
      "Validation samples: 8297\n",
      "Validation targets: 8297\n",
      "Testing samples: 9189\n",
      "Testing targets: 9189\n",
      "Fold 7\n",
      "Training samples: 74595\n",
      "Training targets: 74595\n",
      "Validation samples: 8370\n",
      "Validation targets: 8370\n",
      "Testing samples: 9155\n",
      "Testing targets: 9155\n",
      "Fold 8\n",
      "Training samples: 74733\n",
      "Training targets: 74733\n",
      "Validation samples: 8348\n",
      "Validation targets: 8348\n",
      "Testing samples: 9039\n",
      "Testing targets: 9039\n",
      "Fold 9\n",
      "Training samples: 74493\n",
      "Training targets: 74493\n",
      "Validation samples: 8392\n",
      "Validation targets: 8392\n",
      "Testing samples: 9235\n",
      "Testing targets: 9235\n",
      "Fold 10\n",
      "Training samples: 74693\n",
      "Training targets: 74693\n",
      "Validation samples: 8320\n",
      "Validation targets: 8320\n",
      "Testing samples: 9107\n",
      "Testing targets: 9107\n",
      "4736 4736\n"
     ]
    }
   ],
   "source": [
    "# Access the data for each fold\n",
    "for i, (x_train_fold, y_train_fold, x_val_fold, y_val_fold, x_test_fold, y_test_fold) in enumerate(folds):\n",
    "    print(f\"Fold {i + 1}\")\n",
    "    print(\"Training samples:\", len(x_train_fold))\n",
    "    print(\"Training targets:\", len(y_train_fold))\n",
    "    print(\"Validation samples:\", len(x_val_fold))\n",
    "    print(\"Validation targets:\", len(y_val_fold))\n",
    "    print(\"Testing samples:\", len(x_test_fold))\n",
    "    print(\"Testing targets:\", len(y_test_fold))\n",
    "\n",
    "\n",
    "test_sets_path = '/home/pmedur/strojnoUcenje/env/bin/Tensorflow_speech_recognition/tflite-speech-recognition-master'\n",
    "test_sets_filename = 'further_test_data.npz'\n",
    "test_sets = np.load(join(test_sets_path, test_sets_filename))\n",
    "x_test = test_sets['x_test']\n",
    "y_test = test_sets['y_test']\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d3925c-87d9-4f36-968a-4441e7427373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the .npz file: ['x_test', 'y_test']\n",
      "Shape of x_test: (4736, 16, 16)\n",
      "Shape of y_test: (4736,)\n",
      "First 5 samples from x_test: [[[-8.70189777e+01 -6.48526714e+01 -3.68849118e+01 ... -4.69820656e+01\n",
      "   -4.74103109e+01 -5.28231571e+01]\n",
      "  [-9.87812317e-01  1.84348644e+00  5.28241541e+00 ... -7.23295317e+00\n",
      "   -4.92526404e+00 -1.89042105e+00]\n",
      "  [ 2.81920711e+00  1.05662018e+01  1.06800097e+01 ...  1.17842296e+00\n",
      "    2.63428594e+00  4.35082154e+00]\n",
      "  ...\n",
      "  [-1.50546452e-02  7.66339290e-01 -9.23205255e-01 ... -8.73862684e-01\n",
      "   -5.30805788e-01  9.14999231e-02]\n",
      "  [-1.31747528e+00 -7.49570258e-01  3.75609763e-02 ... -8.38265017e-01\n",
      "   -9.17036677e-01 -5.80532993e-01]\n",
      "  [-5.43645768e-01 -8.43938676e-01 -8.04250199e-01 ...  3.26841319e-01\n",
      "    4.15967951e-01  3.43705418e-01]]\n",
      "\n",
      " [[-1.79392601e+01 -1.50582367e+01 -1.42862228e+01 ... -4.69396647e+01\n",
      "   -4.22841649e+01 -4.36599941e+01]\n",
      "  [ 4.74644730e+00  4.93073893e+00  5.11698782e+00 ... -1.16546350e+01\n",
      "   -1.22623247e+01 -1.21386413e+01]\n",
      "  [ 7.60325533e-01 -2.19739589e+00 -3.25615814e+00 ... -1.22976196e+00\n",
      "   -4.68182754e-01  2.66317125e-02]\n",
      "  ...\n",
      "  [ 5.74492662e-02  4.45224303e-01  8.45303328e-01 ... -3.78494217e-01\n",
      "   -1.72567220e-02 -7.13314081e-02]\n",
      "  [-2.87405748e-02 -1.33397264e-01 -3.59509740e-01 ...  4.83240605e-01\n",
      "    7.42620136e-01  5.56215002e-01]\n",
      "  [-7.23686438e-03 -3.15895270e-01 -5.30514639e-01 ... -1.07583796e+00\n",
      "   -8.41103796e-01 -7.42098839e-01]]\n",
      "\n",
      " [[-3.52720488e+01 -1.78377106e+01 -1.02410280e+01 ... -5.17121549e+01\n",
      "   -4.86540144e+01 -5.06802159e+01]\n",
      "  [ 6.28783432e+00  4.79605570e+00  3.85415835e+00 ... -7.13016248e+00\n",
      "   -3.64070566e+00  5.64984986e-01]\n",
      "  [ 1.01262818e+01  4.26674691e+00  8.79933805e-01 ...  1.84755244e+00\n",
      "    3.39772766e+00  4.50807692e+00]\n",
      "  ...\n",
      "  [-6.48093439e-01 -2.38048333e-01  1.45215843e-01 ... -2.51442685e-01\n",
      "   -2.62777375e-02  6.59229956e-02]\n",
      "  [ 3.48077170e-01 -2.90218478e-01 -8.48617720e-01 ... -6.14244618e-01\n",
      "   -6.56076362e-01 -7.08369009e-01]\n",
      "  [-4.01246480e-01  2.46507769e-02 -1.32684650e-01 ...  1.01193286e-01\n",
      "    4.57999807e-01  2.90537715e-01]]\n",
      "\n",
      " [[-1.02195888e+02 -1.01990434e+02 -1.02553425e+02 ... -1.21677700e+01\n",
      "   -1.45212543e+01 -2.19144979e+01]\n",
      "  [-1.52999156e+00 -1.61505883e+00 -9.07276329e-01 ...  4.89558782e+00\n",
      "    4.52795158e+00  2.75666566e+00]\n",
      "  [ 1.04649636e+00  7.81173592e-01  2.06525214e-01 ... -9.52379510e-01\n",
      "   -3.69672494e-01  1.34726162e+00]\n",
      "  ...\n",
      "  [-3.42412355e-02 -8.27923976e-02  4.84455044e-02 ...  6.04771447e-01\n",
      "    4.10635938e-01  4.48392373e-01]\n",
      "  [ 3.65413087e-02  5.86720548e-02 -1.58708385e-01 ... -8.20947317e-01\n",
      "   -7.16517456e-01 -5.14153458e-01]\n",
      "  [ 4.35528619e-01  5.26163929e-01  1.27354224e-01 ...  2.37043901e-01\n",
      "    1.77650595e-01 -5.37713629e-02]]\n",
      "\n",
      " [[-3.52120734e+01 -2.23054877e+01 -1.60778483e+01 ... -7.55905707e+01\n",
      "   -8.46411134e+01 -8.71606674e+01]\n",
      "  [ 6.99573340e+00  5.14733044e+00  3.97908758e+00 ...  1.20846576e+00\n",
      "    5.15750555e+00  6.24158263e+00]\n",
      "  [ 7.08994535e+00  2.66248872e+00 -4.45441920e-01 ...  2.38265464e+00\n",
      "    1.83267768e+00  3.10696809e+00]\n",
      "  ...\n",
      "  [-1.53462713e+00 -7.15874105e-01  1.90747663e-01 ... -7.99870332e-01\n",
      "   -4.07162490e-01 -1.01698202e-01]\n",
      "  [ 4.12341737e-01 -2.20055854e-01 -4.10499736e-01 ...  5.33163230e-02\n",
      "   -2.01764215e-01  8.47097834e-03]\n",
      "  [-8.51144064e-01 -3.05014900e-01 -2.14335317e-01 ... -2.07548900e-02\n",
      "   -3.19816065e-01 -5.93332302e-01]]]\n",
      "First 5 samples from y_test: [29 29 29 29 29]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "# Load the test data\n",
    "test_sets_path = '/home/pmedur/strojnoUcenje/env/bin/Tensorflow_speech_recognition/tflite-speech-recognition-master'\n",
    "test_sets_filename = 'further_test_data.npz'\n",
    "test_sets = np.load(join(test_sets_path, test_sets_filename))\n",
    "\n",
    "# Print all keys in the .npz file\n",
    "print(\"Keys in the .npz file:\", test_sets.files)\n",
    "\n",
    "# Print the shape of each dataset in the .npz file\n",
    "for key in test_sets.files:\n",
    "    print(f\"Shape of {key}: {test_sets[key].shape}\")\n",
    "\n",
    "# Optionally, print some samples from each dataset\n",
    "for key in test_sets.files:\n",
    "    print(f\"First 5 samples from {key}: {test_sets[key][:5]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
